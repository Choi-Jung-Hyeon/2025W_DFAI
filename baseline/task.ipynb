{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c65c32d",
   "metadata": {},
   "source": [
    "## 딥페이크 범죄 대응을 위한 AI 탐지 모델 경진대회\n",
    "\n",
    "**※주의** : 반드시 본 파일을 이용하여 제출을 수행해야 하며, 파일의 이름은 `task.ipynb`로 유지되어야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376a9781",
   "metadata": {},
   "source": [
    "* #### 추론 실행 환경\n",
    "    * `python 3.9` 환경\n",
    "    * `CUDA 10.2`, `CUDA 11.8`, `CUDA 12.6`를 지원합니다.\n",
    "    * 각 CUDA 환경에 미리 설치돼있는 torch 버전은 다음 표를 참고하세요.\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th align=\"center\">Python</th>\n",
    "      <th align=\"center\">CUDA</th>\n",
    "      <th align=\"center\">torch</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td align=\"center\" style=\"vertical-align: middle;\">3.8</td>\n",
    "      <td align=\"center\">10.2</td>\n",
    "      <td align=\"center\">1.6.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td align=\"center\" style=\"vertical-align: middle;\">3.9</td>\n",
    "      <td align=\"center\">11.8</td>\n",
    "      <td align=\"center\">1.8.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td align=\"center\">3.10</td>\n",
    "      <td align=\"center\">12.6</td>\n",
    "      <td align=\"center\">2.7.1</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3001744a",
   "metadata": {},
   "source": [
    "* #### CUDA 버전 관련 안내사항  \n",
    "  - 이번 경진대회는 3개의 CUDA 버전을 지원합니다.  \n",
    "  - 참가자는 자신의 모델의 라이브러리 의존성에 맞는 CUDA 환경을 선택하여 모델을 제출하면 됩니다.   \n",
    "  - 각 CUDA 환경에는 기본적으로 torch가 설치되어 있으나, 참가자는 제출하는 CUDA 버전과 호환되는 torch, 필요한 버전의 라이브러리를 `!pip install` 하여 사용하여도 무관합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b530b5",
   "metadata": {},
   "source": [
    "* #### `task.ipynb` 작성 규칙\n",
    "코드는 크게 3가지 파트로 구성되며, 해당 파트의 특성을 지켜서 내용을 편집하세요.   \n",
    "1. **제출용 aifactory 라이브러리 및 추가 필요 라이브러리 설치**\n",
    "    - 채점 및 제출을 위한 aifactory 라이브러리를 설치하는 셀입니다. 이 부분은 수정하지 않고 그대로 실행합니다.\n",
    "    - 그 외로, 모델 추론에 필요한 라이브러리를 직접 설치합니다.\n",
    "2. **추론용 코드 작성**\n",
    "    - 모델 로드, 데이터 전처리, 예측 등 실제 추론을 수행하는 모든 코드를 이 영역에 작성합니다.\n",
    "3. **aif.submit() 함수를 호출하여 최종 결과를 제출**\n",
    "    - **마이 페이지-활동히스토리**에서 발급받은 key 값을 함수의 인자로 정확히 입력해야 합니다.\n",
    "    - **※주의** : 제출하고자 하는 CUDA 환경에 맞는 key를 입력하여야 합니다.\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th align=\"left\">Competition 이름</th>\n",
    "      <th align=\"center\">CUDA</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td align=\"left\">딥페이크 범죄 대응을 위한 AI 탐지 모델 경진대회</td>\n",
    "      <td align=\"center\">11.8</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td align=\"left\">딥페이크 범죄 대응을 위한 AI 탐지 모델 경진대회 CUDA 12.6</td>\n",
    "      <td align=\"center\">12.6</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td align=\"left\">딥페이크 범죄 대응을 위한 AI 탐지 모델 경진대회 CUDA 10.2</td>\n",
    "      <td align=\"center\">10.2</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8e0843",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7e2061",
   "metadata": {},
   "source": [
    "#### 1. 제출용 aifactory 라이브러리 설치\n",
    "※ 결과 전송에 필요하므로 아래와 같이 aifactory 라이브러리가 반드시 최신버전으로 설치될 수 있게끔 합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4acd91eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aifactory in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (2.0.0)\n",
      "Requirement already satisfied: pipreqs in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from aifactory) (0.5.0)\n",
      "Requirement already satisfied: ipynbname in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from aifactory) (2025.8.0.0)\n",
      "Requirement already satisfied: gdown in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from aifactory) (5.2.0)\n",
      "Requirement already satisfied: requests in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from aifactory) (2.32.5)\n",
      "Requirement already satisfied: IPython in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from aifactory) (8.12.3)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from gdown->aifactory) (4.14.2)\n",
      "Requirement already satisfied: filelock in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from gdown->aifactory) (3.19.1)\n",
      "Requirement already satisfied: tqdm in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from gdown->aifactory) (4.67.1)\n",
      "Requirement already satisfied: ipykernel in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from ipynbname->aifactory) (7.1.0)\n",
      "Requirement already satisfied: backcall in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from IPython->aifactory) (0.2.0)\n",
      "Requirement already satisfied: decorator in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from IPython->aifactory) (5.2.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from IPython->aifactory) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from IPython->aifactory) (0.2.1)\n",
      "Requirement already satisfied: pickleshare in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from IPython->aifactory) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from IPython->aifactory) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from IPython->aifactory) (2.19.2)\n",
      "Requirement already satisfied: stack-data in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from IPython->aifactory) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from IPython->aifactory) (5.14.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from IPython->aifactory) (4.9.0)\n",
      "Requirement already satisfied: docopt==0.6.2 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from pipreqs->aifactory) (0.6.2)\n",
      "Requirement already satisfied: nbconvert<8.0.0,>=7.11.0 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from pipreqs->aifactory) (7.16.6)\n",
      "Requirement already satisfied: yarg==0.1.9 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from pipreqs->aifactory) (0.1.9)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from requests->aifactory) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from requests->aifactory) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from requests->aifactory) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from requests->aifactory) (2025.10.5)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from jedi>=0.16->IPython->aifactory) (0.8.5)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from bleach[css]!=5.0.0->nbconvert<8.0.0,>=7.11.0->pipreqs->aifactory) (6.3.0)\n",
      "Requirement already satisfied: defusedxml in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs->aifactory) (0.7.1)\n",
      "Requirement already satisfied: jinja2>=3.0 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs->aifactory) (3.1.6)\n",
      "Requirement already satisfied: jupyter-core>=4.7 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs->aifactory) (5.9.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs->aifactory) (0.3.0)\n",
      "Requirement already satisfied: markupsafe>=2.0 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs->aifactory) (2.1.5)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs->aifactory) (3.1.4)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs->aifactory) (0.10.2)\n",
      "Requirement already satisfied: nbformat>=5.7 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs->aifactory) (5.10.4)\n",
      "Requirement already satisfied: packaging in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs->aifactory) (25.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs->aifactory) (1.5.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from pexpect>4.3->IPython->aifactory) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->IPython->aifactory) (0.2.14)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from beautifulsoup4->gdown->aifactory) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from beautifulsoup4->gdown->aifactory) (4.15.0)\n",
      "Requirement already satisfied: comm>=0.1.1 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from ipykernel->ipynbname->aifactory) (0.2.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from ipykernel->ipynbname->aifactory) (1.8.17)\n",
      "Requirement already satisfied: jupyter-client>=8.0.0 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from ipykernel->ipynbname->aifactory) (8.6.3)\n",
      "Requirement already satisfied: nest-asyncio>=1.4 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from ipykernel->ipynbname->aifactory) (1.6.0)\n",
      "Requirement already satisfied: psutil>=5.7 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from ipykernel->ipynbname->aifactory) (7.1.3)\n",
      "Requirement already satisfied: pyzmq>=25 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from ipykernel->ipynbname->aifactory) (27.1.0)\n",
      "Requirement already satisfied: tornado>=6.2 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from ipykernel->ipynbname->aifactory) (6.5.2)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from requests[socks]->gdown->aifactory) (1.7.1)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from stack-data->IPython->aifactory) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from stack-data->IPython->aifactory) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from stack-data->IPython->aifactory) (0.2.3)\n",
      "Requirement already satisfied: webencodings in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert<8.0.0,>=7.11.0->pipreqs->aifactory) (0.5.1)\n",
      "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from bleach[css]!=5.0.0->nbconvert<8.0.0,>=7.11.0->pipreqs->aifactory) (1.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from jupyter-client>=8.0.0->ipykernel->ipynbname->aifactory) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from jupyter-core>=4.7->nbconvert<8.0.0,>=7.11.0->pipreqs->aifactory) (4.5.0)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from nbformat>=5.7->nbconvert<8.0.0,>=7.11.0->pipreqs->aifactory) (2.21.2)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from nbformat>=5.7->nbconvert<8.0.0,>=7.11.0->pipreqs->aifactory) (4.25.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert<8.0.0,>=7.11.0->pipreqs->aifactory) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert<8.0.0,>=7.11.0->pipreqs->aifactory) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert<8.0.0,>=7.11.0->pipreqs->aifactory) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert<8.0.0,>=7.11.0->pipreqs->aifactory) (0.28.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->jupyter-client>=8.0.0->ipykernel->ipynbname->aifactory) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U aifactory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0deb93f",
   "metadata": {},
   "source": [
    "* 자신의 모델 추론 실행에 필요한 추가 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf6ceb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (8.3.227)\n",
      "Requirement already satisfied: numpy>=1.23.0 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from ultralytics) (2.2.6)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from ultralytics) (3.10.7)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from ultralytics) (4.12.0.88)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from ultralytics) (12.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from ultralytics) (6.0.3)\n",
      "Requirement already satisfied: requests>=2.23.0 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from ultralytics) (2.32.5)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from ultralytics) (1.16.3)\n",
      "Requirement already satisfied: torch>=1.8.0 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from ultralytics) (2.7.1+cu118)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from ultralytics) (0.22.1+cu118)\n",
      "Requirement already satisfied: psutil in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from ultralytics) (7.1.3)\n",
      "Requirement already satisfied: polars in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from ultralytics) (1.35.2)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.18 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from ultralytics) (2.0.18)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (2025.10.5)\n",
      "Requirement already satisfied: filelock in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (70.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (11.8.86)\n",
      "Requirement already satisfied: triton==3.3.1 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (3.3.1)\n",
      "Requirement already satisfied: polars-runtime-32==1.35.2 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from polars->ultralytics) (1.35.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n",
      "Collecting transformers==4.30\n",
      "  Using cached transformers-4.30.0-py3-none-any.whl.metadata (113 kB)\n",
      "Requirement already satisfied: filelock in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from transformers==4.30) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from transformers==4.30) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from transformers==4.30) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from transformers==4.30) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from transformers==4.30) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from transformers==4.30) (2025.11.3)\n",
      "Requirement already satisfied: requests in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from transformers==4.30) (2.32.5)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.30)\n",
      "  Using cached tokenizers-0.13.3.tar.gz (314 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: safetensors>=0.3.1 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from transformers==4.30) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from transformers==4.30) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from requests->transformers==4.30) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from requests->transformers==4.30) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from requests->transformers==4.30) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from requests->transformers==4.30) (2025.10.5)\n",
      "Using cached transformers-4.30.0-py3-none-any.whl (7.2 MB)\n",
      "Building wheels for collected packages: tokenizers\n",
      "  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[62 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m /tmp/pip-build-env-0dykdlyc/overlay/lib/python3.12/site-packages/setuptools/dist.py:759: SetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m         Please consider removing the following classifiers in favor of a SPDX license expression:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         License :: OSI Approved :: Apache Software License\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m   self._finalize_license_expression()\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-312/tokenizers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/__init__.py -> build/lib.linux-x86_64-cpython-312/tokenizers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-312/tokenizers/models\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/models/__init__.py -> build/lib.linux-x86_64-cpython-312/tokenizers/models\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-312/tokenizers/decoders\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/decoders/__init__.py -> build/lib.linux-x86_64-cpython-312/tokenizers/decoders\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-312/tokenizers/normalizers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/normalizers/__init__.py -> build/lib.linux-x86_64-cpython-312/tokenizers/normalizers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-312/tokenizers/pre_tokenizers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/pre_tokenizers/__init__.py -> build/lib.linux-x86_64-cpython-312/tokenizers/pre_tokenizers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-312/tokenizers/processors\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/processors/__init__.py -> build/lib.linux-x86_64-cpython-312/tokenizers/processors\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-312/tokenizers/trainers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/trainers/__init__.py -> build/lib.linux-x86_64-cpython-312/tokenizers/trainers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/bert_wordpiece.py -> build/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/sentencepiece_bpe.py -> build/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/base_tokenizer.py -> build/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/char_level_bpe.py -> build/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/sentencepiece_unigram.py -> build/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/byte_level_bpe.py -> build/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/__init__.py -> build/lib.linux-x86_64-cpython-312/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-312/tokenizers/tools\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/tools/visualizer.py -> build/lib.linux-x86_64-cpython-312/tokenizers/tools\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/tools/__init__.py -> build/lib.linux-x86_64-cpython-312/tokenizers/tools\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/__init__.pyi -> build/lib.linux-x86_64-cpython-312/tokenizers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/models/__init__.pyi -> build/lib.linux-x86_64-cpython-312/tokenizers/models\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/decoders/__init__.pyi -> build/lib.linux-x86_64-cpython-312/tokenizers/decoders\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/normalizers/__init__.pyi -> build/lib.linux-x86_64-cpython-312/tokenizers/normalizers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/pre_tokenizers/__init__.pyi -> build/lib.linux-x86_64-cpython-312/tokenizers/pre_tokenizers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/processors/__init__.pyi -> build/lib.linux-x86_64-cpython-312/tokenizers/processors\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/trainers/__init__.pyi -> build/lib.linux-x86_64-cpython-312/tokenizers/trainers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/tools/visualizer-styles.css -> build/lib.linux-x86_64-cpython-312/tokenizers/tools\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m running build_rust\n",
      "  \u001b[31m   \u001b[0m error: can't find Rust compiler\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m To update pip, run:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m     pip install --upgrade pip\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m and then retry package installation.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n",
      "\u001b[0mFailed to build tokenizers\n",
      "\u001b[31mERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
      "\u001b[0mLooking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch==2.7.1 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (2.7.1+cu118)\n",
      "Requirement already satisfied: torchvision==0.22.1 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (0.22.1+cu118)\n",
      "Requirement already satisfied: filelock in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from torch==2.7.1) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from torch==2.7.1) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from torch==2.7.1) (70.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from torch==2.7.1) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from torch==2.7.1) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from torch==2.7.1) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from torch==2.7.1) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from torch==2.7.1) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from torch==2.7.1) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from torch==2.7.1) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from torch==2.7.1) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from torch==2.7.1) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from torch==2.7.1) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from torch==2.7.1) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from torch==2.7.1) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from torch==2.7.1) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from torch==2.7.1) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from torch==2.7.1) (11.8.86)\n",
      "Requirement already satisfied: triton==3.3.1 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from torch==2.7.1) (3.3.1)\n",
      "Requirement already satisfied: numpy in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from torchvision==0.22.1) (2.2.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from torchvision==0.22.1) (12.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch==2.7.1) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from jinja2->torch==2.7.1) (2.1.5)\n",
      "Requirement already satisfied: opencv-python-headless in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (4.12.0.88)\n",
      "Requirement already satisfied: pillow in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (12.0.0)\n",
      "Requirement already satisfied: numpy in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (2.2.6)\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.3.4-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: pandas in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (2.3.3)\n",
      "Requirement already satisfied: scikit-learn in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (1.7.2)\n",
      "Requirement already satisfied: tqdm in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/fourmi103/2025W_DFAI/venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# dlib 설치 라인 제거\n",
    "# !pip install -U dlib \n",
    "\n",
    "# YOLOv8 라이브러리 설치\n",
    "!pip install -U ultralytics\n",
    "\n",
    "# 베이스라인 모델(ViT)을 위한 Transformers 설치\n",
    "!pip install transformers==4.30\n",
    "\n",
    "# 대회 추론 환경(CUDA 11.8)에 맞는 PyTorch 설치\n",
    "!pip install -U torch==2.7.1 torchvision==0.22.1 --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# 기타 필요한 라이브러리\n",
    "!pip install -U opencv-python-headless pillow numpy pandas scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19999e24",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf1b43e",
   "metadata": {},
   "source": [
    "#### 2. 추론용 코드 작성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1d194e",
   "metadata": {},
   "source": [
    "##### 추론 환경의 기본 경로 구조\n",
    "\n",
    "- 평가 데이터셋 경로: `./data/`\n",
    "   - 채점에 사용될 테스트 데이터셋은 `./data/` 디렉토리 안에 포함되어 있습니다.\n",
    "   - 해당 디렉토리에는 이미지(JPG, PNG)와 동영상(MP4) 파일이 별도의 하위 폴더 없이 혼합되어 있습니다.\n",
    "```bash\n",
    "/aif/\n",
    "└── data/\n",
    "    ├── {이미지 데이터1}.jpg\n",
    "    ├── {이미지 데이터2}.png\n",
    "    ├── {동영상 데이터1}.mp4\n",
    "    ├── {이미지 데이터3}.png\n",
    "    ├── {동영상 데이터2}.mp4\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335719c1",
   "metadata": {},
   "source": [
    "- 모델 및 자원 경로: 예시 : `./model/`\n",
    "   - 추론 스크립트가 실행되는 위치를 기준으로, 제출된 모델 관련 파일들이 위치해야하 하는 상대 경로입니다.\n",
    "   - 학습된 모델 가중치(.pt, .ckpt, .pth 등)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c1810d",
   "metadata": {},
   "source": [
    "* 제출 파일은 `submission.csv`로 저장돼야 합니다.\n",
    "  * submission.csv는 *filename*과 *label* 컬럼으로 구성돼야 합니다.\n",
    "  * filename은 추론한 파일의 이름(확장자 포함), label은 추론 결과입니다. (real:0, fake:1)\n",
    "  * filename은 *string*, label은 *int* 자료형이어야 합니다.\n",
    "  * 추론하는 데이터의 순서는 무작위로 섞여도 상관 없습니다.\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th align=\"center\">filename</th>\n",
    "      <th align=\"center\">label</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td align=\"center\">{이미지 데이터1}.jpg</td>\n",
    "      <td align=\"center\">0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td align=\"center\">{동영상 데이터1}.mp4</td>\n",
    "      <td align=\"center\">1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td colspan=\"2\" align=\"center\">...</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c29c10f",
   "metadata": {},
   "source": [
    "**※ 주의 사항**\n",
    "\n",
    "* argparse 사용시 `args, _ = parser.parse_known_args()`로 인자를 지정하세요.   \n",
    "   - `args = parser.parse_args()`는 jupyter에서 오류가 발생합니다.\n",
    "* return 할 결과물과 양식에 유의하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff1c36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fatal error during model loading: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 60\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFatal error during model loading: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m# 모델 로드 실패 시, 추론이 불가능하므로 여기서 중단합니다.\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# (실제 제출 시에는 이 print문이 에러 로그에 남게 됩니다.)\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m####################################################################\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# 유틸리티 함수 (YOLOv8-Face 맞춤형)\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m####################################################################\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_boundingbox_yolo\u001b[39m(box, width, height):\n",
      "Cell \u001b[0;32mIn[5], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# 1. YOLOv8-Face 얼굴 탐지기 로드 (GPU 사용)\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     face_detector \u001b[38;5;241m=\u001b[39m YOLO(yolo_weights)\n\u001b[0;32m---> 47\u001b[0m     \u001b[43mface_detector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYOLOv8-Face detector loaded successfully from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00myolo_weights\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and moved to GPU.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# 2. 베이스라인 ViT 모델 로드 (GPU 사용)\u001b[39;00m\n",
      "File \u001b[0;32m~/2025W_DFAI/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1355\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1353\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/2025W_DFAI/venv/lib/python3.12/site-packages/ultralytics/engine/model.py:856\u001b[0m, in \u001b[0;36mModel._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    835\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Apply a function to model tensors that are not parameters or registered buffers.\u001b[39;00m\n\u001b[1;32m    836\u001b[0m \n\u001b[1;32m    837\u001b[0m \u001b[38;5;124;03mThis method extends the functionality of the parent class's _apply method by additionally resetting the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    853\u001b[0m \u001b[38;5;124;03m    >>> model = model._apply(lambda t: t.cuda())  # Move model to GPU\u001b[39;00m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_is_pytorch_model()\n\u001b[0;32m--> 856\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# reset predictor as device may have changed\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverrides[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice  \u001b[38;5;66;03m# was str(self.device) i.e. device(type='cuda', index=0) -> 'cuda:0'\u001b[39;00m\n",
      "File \u001b[0;32m~/2025W_DFAI/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/2025W_DFAI/venv/lib/python3.12/site-packages/ultralytics/nn/tasks.py:281\u001b[0m, in \u001b[0;36mBaseModel._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    273\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply a function to all tensors in the model that are not parameters or registered buffers.\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;124;03m        (BaseModel): An updated BaseModel object.\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 281\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m     m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Detect()\u001b[39;00m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    284\u001b[0m         m, Detect\n\u001b[1;32m    285\u001b[0m     ):  \u001b[38;5;66;03m# includes all Detect subclasses like Segment, Pose, OBB, WorldDetect, YOLOEDetect, YOLOESegment\u001b[39;00m\n",
      "File \u001b[0;32m~/2025W_DFAI/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/2025W_DFAI/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/2025W_DFAI/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/2025W_DFAI/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:942\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 942\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    943\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    945\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/2025W_DFAI/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1341\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1336\u001b[0m             device,\n\u001b[1;32m   1337\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1338\u001b[0m             non_blocking,\n\u001b[1;32m   1339\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1340\u001b[0m         )\n\u001b[0;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1345\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/2025W_DFAI/venv/lib/python3.12/site-packages/torch/cuda/__init__.py:372\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    371\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 372\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    376\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "# =============================================================\n",
    "# 2번 셀: 추론 코드 (CUDA용)\n",
    "# =============================================================\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import ViTForImageClassification, ViTImageProcessor\n",
    "from ultralytics import YOLO\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "### 추론 환경 경로 설정\n",
    "# (주의!) 지금은 베이스라인 ViT 모델을 사용합니다.\n",
    "# (STEP 4) DINOv2 학습 완료 후, 이 경로를\n",
    "# 1위 모델이 저장된 폴더(예: \"./models_trained/dino_v2_4channel\")로 변경해야 합니다.\n",
    "model_path = \"./model/deep-fake-detector-v2-model\"\n",
    "\n",
    "# (중요!) 파일 이름 확인\n",
    "yolo_weights = \"yolov8n-face-lindevs.pt\" \n",
    "\n",
    "# 테스트 데이터가 저장된 경로 (규칙이므로 변경 금지)\n",
    "test_data_path = \"./data/\"\n",
    "submission_file = \"submission.csv\"\n",
    "\n",
    "### 상수 정의\n",
    "IMAGE_EXTS = {\".jpg\", \".jpeg\", \".png\"}\n",
    "VIDEO_EXTS = {\".avi\", \".mp4\"}\n",
    "TARGET_SIZE = (224, 224)\n",
    "NUM_FRAMES_TO_EXTRACT = 30\n",
    "\n",
    "####################################################################\n",
    "# 유틸리티 함수\n",
    "####################################################################\n",
    "\n",
    "def get_boundingbox_yolo(box, width, height):\n",
    "    \"\"\"YOLO의 [x1, y1, x2, y2] 포맷을 dlib과 유사한 정사각형 박스로 변환합니다.\"\"\"\n",
    "    x1, y1, x2, y2 = int(box[0]), int(box[1]), int(box[2]), int(box[3])\n",
    "    size_bb = int(max(x2 - x1, y2 - y1) * 1.3)\n",
    "    center_x, center_y = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "    x1 = max(int(center_x - size_bb // 2), 0)\n",
    "    y1 = max(int(center_y - size_bb // 2), 0)\n",
    "    size_bb = min(width - x1, size_bb)\n",
    "    size_bb = min(height - y1, size_bb)\n",
    "    return x1, y1, size_bb\n",
    "\n",
    "def detect_and_crop_face_optimized(image: Image.Image, face_detector, target_size=TARGET_SIZE):\n",
    "    \"\"\"(수정!) face_detector 객체를 파라미터로 받습니다.\"\"\"\n",
    "    if image.mode != 'RGB': image = image.convert('RGB')\n",
    "    \n",
    "    # YOLO 모델로 얼굴 탐지 (GPU 사용)\n",
    "    results = face_detector(image, device=\"cuda\", verbose=False)\n",
    "\n",
    "    if not results or len(results[0].boxes) == 0:\n",
    "        return None  \n",
    "\n",
    "    original_np = np.array(image)\n",
    "    original_h, original_w, _ = original_np.shape\n",
    "    best_box = results[0].boxes.xyxy[0].cpu().numpy() \n",
    "    \n",
    "    x, y, size = get_boundingbox_yolo(best_box, original_w, original_h)\n",
    "    cropped_np = original_np[y:y + size, x:x + size]\n",
    "    \n",
    "    if cropped_np.size == 0:\n",
    "        return None\n",
    "        \n",
    "    face_img = Image.fromarray(cropped_np).resize(target_size, Image.BICUBIC)\n",
    "    return face_img\n",
    "\n",
    "####################################################################\n",
    "# 메인 처리 함수 (파일 1개 처리)\n",
    "####################################################################\n",
    "def process_single_file(file_path, face_detector): # <-- (수정!) face_detector 전달받음\n",
    "    \"\"\"\n",
    "    파일 경로(Path 객체)를 입력받아, 크롭된 얼굴 이미지 리스트를 반환합니다.\n",
    "    \"\"\"\n",
    "    face_images = []\n",
    "    ext = file_path.suffix.lower()\n",
    "\n",
    "    try:\n",
    "        if ext in IMAGE_EXTS:\n",
    "            image = Image.open(file_path)\n",
    "            face_img = detect_and_crop_face_optimized(image, face_detector) # <-- (수정!)\n",
    "            if face_img:\n",
    "                face_images.append(face_img)\n",
    "                \n",
    "        elif ext in VIDEO_EXTS:\n",
    "            cap = cv2.VideoCapture(str(file_path))\n",
    "            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            \n",
    "            if total_frames > 0:\n",
    "                frame_indices = np.linspace(0, total_frames - 1, NUM_FRAMES_TO_EXTRACT, dtype=int)\n",
    "                for idx in frame_indices:\n",
    "                    cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "                    ret, frame = cap.read()\n",
    "                    if not ret: continue\n",
    "                    image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "                    face_img = detect_and_crop_face_optimized(image, face_detector) # <-- (수정!)\n",
    "                    if face_img:\n",
    "                        face_images.append(face_img)\n",
    "            cap.release()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path.name}: {e}\")\n",
    "        return [] \n",
    "\n",
    "    return face_images\n",
    "\n",
    "####################################################################\n",
    "# 메인 실행 로직\n",
    "####################################################################\n",
    "if __name__ == \"__main__\": # <--- (수정!) [multiprocessing 공지사항] 준수\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # --- (수정!) 모델 로드를 __main__ 안으로 이동 ---\n",
    "    try:\n",
    "        face_detector = YOLO(yolo_weights)\n",
    "        face_detector.to(\"cuda\")\n",
    "        print(f\"YOLOv8-Face detector loaded successfully from {yolo_weights}.\")\n",
    "\n",
    "        model = ViTForImageClassification.from_pretrained(model_path).to(\"cuda\")\n",
    "        processor = ViTImageProcessor.from_pretrained(model_path)\n",
    "        model.eval()\n",
    "        print(f\"ViT model loaded successfully from {model_path}.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Fatal error during model loading: {e}\")\n",
    "        raise e\n",
    "    # --- 모델 로드 종료 ---\n",
    "\n",
    "    data_dir = Path(test_data_path)\n",
    "    files = [f for f in data_dir.iterdir() if f.is_file() and f.suffix.lower() in (IMAGE_EXTS | VIDEO_EXTS)]\n",
    "    print(f\"Found {len(files)} files in {test_data_path}\")\n",
    "    results_to_write = {}\n",
    "\n",
    "    print(\"Starting inference using single process (YOLO on GPU)...\")\n",
    "    \n",
    "    with tqdm(total=len(files), desc=\"Processing files\") as pbar:\n",
    "        for f in files:\n",
    "            filename = f.name\n",
    "            \n",
    "            # (수정!) face_detector 객체 전달\n",
    "            face_images = process_single_file(f, face_detector) \n",
    "            \n",
    "            if not face_images:\n",
    "                results_to_write[filename] = 0 \n",
    "            else:\n",
    "                try:\n",
    "                    inputs = processor(images=face_images, return_tensors=\"pt\").to(\"cuda\")\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(**inputs)\n",
    "                        logits = outputs.logits\n",
    "                        probs = F.softmax(logits, dim=1)\n",
    "                    \n",
    "                    avg_probs = probs.mean(dim=0) \n",
    "                    predicted_class = torch.argmax(avg_probs).item()\n",
    "                    results_to_write[filename] = predicted_class\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error during inference for {filename}: {e}\")\n",
    "                    results_to_write[filename] = 0 \n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "    print(f\"Inference completed. Saving results to {submission_file}...\")\n",
    "    submission_df = pd.DataFrame(results_to_write.items(), columns=[\"filename\", \"label\"])\n",
    "    submission_df.to_csv(submission_file, index=False)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Total processing time: {end_time - start_time:.2f} seconds\")\n",
    "    print(\"Inference completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6119d40d",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f895e41",
   "metadata": {},
   "source": [
    "#### 3. `aif.submit()` 함수를 호출하여 최종 결과를 제출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8410d97e",
   "metadata": {},
   "source": [
    "**※주의** : task별, 참가자별로 key가 다릅니다. 잘못 입력하지 않도록 유의바랍니다.\n",
    "- key는 대회 페이지 [베이스라인 코드](https://aifactory.space/task/9197/baseline) 탭에 기재된 가이드라인을 따라 task 별로 확인하실 수 있습니다.\n",
    "- key가 틀리면 제출이 진행되지 않거나 잘못 제출되므로 task에 맞는 자신의 key를 사용해야 합니다.\n",
    "-  **NOTE** : 이번 경진대회에서는 3개의 CUDA 버전을 지원하며, 각 CUDA 버전에 따라 task key가 상이합니다. 함수를 실행하기 전에 현재 key가 제출하고자 하는 CUDA 환경에 대한 key인지 반드시 확인하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4df34c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file : task\n",
      "jupyter notebook\n",
      "제출 완료\n",
      "39.245360374450684\n"
     ]
    }
   ],
   "source": [
    "import aifactory.score as aif\n",
    "import time\n",
    "t = time.time()\n",
    "\n",
    "#-----------------------------------------------------#\n",
    "aif.submit(model_name=\"T1_YOLO_ViT_v1\",\n",
    "    key=\"01171c4a-df40-4271-baf0-3a240cbf06a4\"\n",
    ")\n",
    "#-----------------------------------------------------#\n",
    "print(time.time() - t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
